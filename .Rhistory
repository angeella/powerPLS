out <- data.frame(RV = RV, RLS = RLS)
return(out)
}
outX <- similarityMatrix(X = apply(X,2, function(x) scale(x, scale = FALSE)), Y = out$X_H1)
outX
plot(density(X[,1]))
plot(density(out$X_H1[,1]))
library(simrel)
sim_obj <-
simrel(
n      = 50,         # 100 obesrvations
p      = 10,          # predictor variables
q      = 10,           # relevant predictor variables
R2     = 0.9,         # 80% of variation is explained by the model
relpos = c(1, 2),     # Relevant principal components
gamma  = 0.2,         # decay factor of eigenvalue of X
type   = "univariate" # Univariate linear model data simulation
)
X = sim_obj$X
Y = sim_obj$Y
out <- mdatools::plsda(x= sim_obj$X, c = as.factor(Y),
ncomp = 8, cv = 1) #out$ncomp.selected = 4
summary(out)
out <- PLSc(X = X, Y = Y, A =2, scaling = "auto-scaling",
post.transformation = TRUE, eps = 0.01, Y.prob = FALSE)
X
Y
Y <- ifelse(sim_obj$Y > 0, 1, 0)
Y
out <- PLSc(X = X, Y = Y, A =2, scaling = "auto-scaling",
post.transformation = TRUE, eps = 0.01, Y.prob = FALSE)
out
out$M
out <- PLSc(X = X, Y = Y, A =4, scaling = "auto-scaling",
post.transformation = TRUE, eps = 0.01, Y.prob = FALSE)
out <- sim_XY(out = out, n = nrow(sim_obj$X), seed = 123,
post.transformation = TRUE, A = 4, Y.prob = TRUE)
out <- sim_XY(out = out, n = nrow(sim_obj$X), seed = 123,
post.transformation = TRUE, A = 4)
out
plot(density(sim_obj$X[,3]))
plot(density(out$X_H1[,3]))
plot(density(sim_obj$X[,3]))
plot(density(out$X_H1[,3]))
outX <- similarityMatrix(X = apply(sim_obj$X,2, function(x) scale(x, scale = FALSE)), Y = out$X_H1)
outX
out <- PLSc(X = X, Y = Y, A =4, scaling = "auto-scaling",
post.transformation = TRUE, eps = 0.01, Y.prob = FALSE)
out$M
sim_obj <-
simrel(
n      = 50,         # 100 obesrvations
p      = 10,          # predictor variables
q      = 3,           # relevant predictor variables
R2     = 0.9,         # 80% of variation is explained by the model
relpos = c(1, 2),     # Relevant principal components
gamma  = 0.2,         # decay factor of eigenvalue of X
type   = "univariate" # Univariate linear model data simulation
)
X = sim_obj$X
Y = sim_obj$Y
Y <- ifelse(sim_obj$Y > 0, 1, 0)
out <- PLSc(X = X, Y = Y, A =4, scaling = "auto-scaling",
post.transformation = TRUE, eps = 0.01, Y.prob = FALSE)
out$M
out <- PLSc(X = X, Y = Y, A =5, scaling = "auto-scaling",
post.transformation = TRUE, eps = 0.01, Y.prob = FALSE)
out$M
out <- twoClassSim(n = 30,
intercept = -5, #The intercept, which controls the class balance. The default value (5) produces a roughly balanced data set when the other defaults are used.
linearVars = 3, #The number of linearly important effects.
noiseVars = 3, #The number of uncorrelated irrelevant predictors to be included.
corrVars = 0, #The number of correlated irrelevant predictors to be included.
corrType = "AR1", #The correlation structure of the correlated irrelevant predictors.
corrValue = 0, #The correlation value.
mislabel = 0) #The proportion of data that is possibly mislabeled.
splom(~out[, c(1:(ncol(out)-1))], groups = out$Class)
library(dplyr)
X<- out %>% dplyr::select(-"Class")
Y <- out %>% dplyr::select("Class")
out <- PLSc(X = X, Y = Y, A =3, scaling = "auto-scaling",
post.transformation = TRUE, eps = 0.01, Y.prob = FALSE)
out$M
out <- PLSc(X = X, Y = Y, A =5, scaling = "auto-scaling",
post.transformation = TRUE, eps = 0.01, Y.prob = FALSE)
out$M
out <- sim_XY(out = out, n = nrow(X), seed = 123,
post.transformation = TRUE, A = 3)
out <- sim_XY(out = out, n = nrow(X), seed = 123,
post.transformation = TRUE, A = 5)
outX <- similarityMatrix(X = apply(X,2, function(x) scale(x, scale = FALSE)), Y = out$X_H1)
outX
plot(density(X[,1]))
plot(density(out$X_H1[,1]))
out <- computePower(X = X, Y = Y, A = 4,
post.transformation = TRUE,
seed = 123, scaling = "auto-scaling",
Y.prob = FALSE)
seq(10,20)
n <- c(10,20)
seq(n)
seq(min(n), max(n))
roxygen2::roxygenize()
library(powerPLS)
library(caret)
out <- twoClassSim(n = 30,
intercept = -5, #The intercept, which controls the class balance. The default value (5) produces a roughly balanced data set when the other defaults are used.
linearVars = 3, #The number of linearly important effects.
noiseVars = 3, #The number of uncorrelated irrelevant predictors to be included.
corrVars = 0, #The number of correlated irrelevant predictors to be included.
corrType = "AR1", #The correlation structure of the correlated irrelevant predictors.
corrValue = 0, #The correlation value.
mislabel = 0) #The proportion of data that is possibly mislabeled.
X<- out %>% dplyr::select(-"Class")
Y <- out %>% dplyr::select("Class")
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30),
post.transformation = TRUE,
seed = 123, scaling = "auto-scaling",
Y.prob = FALSE)
roxygen2::roxygenise()
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30),
post.transformation = TRUE,
seed = 123, scaling = "auto-scaling",
Y.prob = FALSE)
n = c(10,30)
A = 4
post.transformation = TRUE
scaling = "auto-scaling"
seed = 123
Y.prob = FALSE
i = min(n)
n
computePower(X = X, Y = Y, A = A, scaling = scaling,
post.transformation = post.transformation,
seed = seed, n = i, Nsim = Nsim, nperm = nperm,
alpha = alpha)
computePower(X = X, Y = Y, A = A, scaling = scaling,
post.transformation = post.transformation,
seed = seed, n = i, Nsim = Nsim, nperm = nperm,
alpha = alpha, Y.prob=FALSE)
Nsim = 1000
Nsim = 100
nperm = 100
computePower(X = X, Y = Y, A = A, scaling = scaling,
post.transformation = post.transformation,
seed = seed, n = i, Nsim = Nsim, nperm = nperm,
alpha = alpha, Y.prob=FALSE)
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30),
post.transformation = TRUE, Nsim = 100, nperm = 100,
seed = 123, scaling = "auto-scaling",
Y.prob = FALSE)
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30), alpha = 0.05, beta = 0.8,<
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30), alpha = 0.05, beta = 0.8,
post.transformation = TRUE, Nsim = 100, nperm = 100,
seed = 123, scaling = "auto-scaling",
Y.prob = FALSE)
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30), alpha = 0.05, beta = 0.8,
post.transformation = TRUE, Nsim = 100, nperm = 100,
seed = 123, scaling = "auto-scaling", Y.prob = FALSE)
0.49+0.49-0.3
0.49+(1-0.49)-0.19
120.8
120*0.8
12*0.8
x <- c(18,22,21,17,16,20,19,21,20,16,27,29)
x <- c(18,22,21,17,16,20,19,21,20,16,27,19)
sort(x)
install.packages("clusterlav")
install.packages("clusterlab")
remotes::install_github("crj32/Clusterlab")
1/6+1/6
1/6*1/6
1- pnorm(0.012)
(90- 90.197)/(4/sqrt(12))
(90- 90.167)/(4/sqrt(12))
library(clusterlab)
k5 <- clusterlab(centers=5)
synthetic <- clusterlab(centers=4,r=8,sdvec=c(2.5,2.5,2.5,2.5),
)
synthetic <- clusterlab(centers=4,r=8,sdvec=c(2.5,2.5,2.5,2.5),
alphas=c(1,1,1,1),centralcluster=FALSE,
numbervec=c(50,50,50,50)) # for a six cluster solution)
install.packages("fungible")
library(fungible)
my.iris <- monte(seed=123, nvar = 4, nclus = 3, cor.list = cormat,
clus.size = c(50, 50, 50),
eta2=c(0.619, 0.401, 0.941, 0.929),
random.cor = FALSE,
skew.list = sk.lst,
kurt.list = kt.lst,
secor = .3, compactness=c(1, 1, 1),
sortMeans = TRUE)
my.iris <- monte(seed=123, nvar = 4, nclus = 3,
clus.size = c(50, 50, 50),
eta2=c(0.619, 0.401, 0.941, 0.929),
random.cor = FALSE,
skew.list = sk.lst,
kurt.list = kt.lst,
secor = .3, compactness=c(1, 1, 1),
sortMeans = TRUE)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.619, 0.401, 0.941, 0.929),
random.cor = FALSE,
secor = .3, compactness=c(1, 1),
sortMeans = TRUE)
my.iris
plot(my.iris)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=0.6, # Higher numbers produce clusters with greater separation on that indicator.
random.cor = FALSE,
secor = .3, compactness=c(1, 1),
sortMeans = TRUE)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0, 0), # Higher numbers produce clusters with greater separation on that indicator.
random.cor = FALSE,
secor = .3, compactness=c(1, 1),
sortMeans = TRUE)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
random.cor = FALSE,
secor = .3, compactness=c(1, 1),
sortMeans = TRUE)
plot(my.iris)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
random.cor = TRUE,
secor = .3, compactness=c(1, 1),
sortMeans = TRUE)
plot(my.iris)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
random.cor = FALSE,
secor = .3, compactness=c(1, 1),
sortMeans = TRUE)
plot(my.iris)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
secor = .5, compactness=c(1, 1),
sortMeans = TRUE)
plot(my.iris)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
secor = .1, compactness=c(1, 1),
sortMeans = TRUE)
plot(my.iris)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(0, 1), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = TRUE)
plot(my.iris)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(0.5, 1), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = TRUE)
plot(my.iris)
my.iris <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(0.5, 1), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
plot(my.iris)
sim <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(1, 1), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
plot(sim)
sim <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(0.1, 1), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
plot(sim)
sim <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(0.1, 0.4), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
plot(sim)
sim <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(0.1, 0.1), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
plot(sim)
sim <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(2, 0.1), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
plot(sim)
sim <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.6, 0.6, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(2, 2), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
plot(sim)
sim <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.8, 0.8, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(2, 2), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
plot(sim)
sim$data
X<- sim$data[[-"id"]]
X<- sim$data[[-c("id")]]
X<- sim$data[, -1]
Y <- sim$data[,1]
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30), alpha = 0.05, beta = 0.8,
post.transformation = TRUE, Nsim = 100, nperm = 100,
seed = 123, scaling = "auto-scaling", Y.prob = FALSE)
library(fungible)
sim <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.8, 0.8, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(1, 1), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
#eta2 is my effect size
plot(sim)
X<- sim$data[, -1]
Y <- sim$data[,1]
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30),
alpha = 0.05, beta = 0.8,
post.transformation = TRUE, Nsim = 100,
nperm = 100,
seed = 123,
scaling = "auto-scaling", Y.prob = FALSE)
A = 4
n = c(10,30)
alpha = 0.05
beta = 0.8
post.transformation = TRUE
Nsim = 100
nperm = 100
seed = 123
scaling = "auto-scaling"
Y.prob = FALSE
nY <- ifelse(is.null(dim(Y)), length(Y), dim(Y)[1])
nY
if(dim(X)[1] != nY){
stop("X and Y must have the same number of observations!")
}
if(!(scaling %in% c("auto-scaling", "pareto-scaling", "mean-centering"))){
stop("available scaling are auto-scaling, pareto-scaling and mean-centering")
}
X<-as.matrix(X)
if(scaling == "auto-scaling"){
Mm <- apply(X, 2, mean)
s <- apply(X, 2, sd)
X <- (X - Mm)/s
}
if(scaling == "pareto-scaling"){
Mm <- 0
s <- apply(X, 2, sd)
X <- (X - Mm)/s
}
if(scaling == "mean-centering"){
Mm <- apply(X, 2, mean)
s <- 1
X <- (X - Mm)/s
}
#If Y is not probability but a vector of classes
if(!Y.prob){
if(is.null(dim(Y)) | ncol(as.matrix(Y))==1){
Y <- as.matrix(Y)
if(!is.factor(Y)){
Y <- as.factor(Y)
}
Y <- model.matrix(~0+Y)
}
#Transform to probability matrix
Y[which(Y==0)]<-eps
Y[which(Y==1)]<-1-(ncol(Y)-1)*eps
#Centered log ratio transform transformation
P <- matrix(clr(Y), ncol = ncol(Y))
}else{
P <- Y
}
eps = 0.01
#If Y is not probability but a vector of classes
if(!Y.prob){
if(is.null(dim(Y)) | ncol(as.matrix(Y))==1){
Y <- as.matrix(Y)
if(!is.factor(Y)){
Y <- as.factor(Y)
}
Y <- model.matrix(~0+Y)
}
#Transform to probability matrix
Y[which(Y==0)]<-eps
Y[which(Y==1)]<-1-(ncol(Y)-1)*eps
#Centered log ratio transform transformation
P <- matrix(clr(Y), ncol = ncol(Y))
}else{
P <- Y
}
#scaling Y
Mm <- apply(P, 2, mean)
s <- apply(P, 2, sd)
P <- (P - Mm)/s
n <- nrow(X)
P <- as.matrix(P)
X <- as.matrix(X)
out <- computeWT(X = X, Y = P, A = A)
W <- out$W
T.score <- out$T.score
R <- out$R
if(post.transformation){
out <- ptPLSc(X = X, Y = matrix(clr(Y), ncol = ncol(Y)), W = W)
Wtilde <- out$Wtilde
M <- out$M
#apply G to weight matrix
N <- dim(X)[1]
E <- r <- Q <- list()
E[[1]] <- X
T.score <- IDA(X = X, Y = Y, W = Wtilde)
}else{
M <- NULL
}
M
#Compute loadings matrix
X.loading = t(X) %*% T.score %*% solve(t(T.score) %*% T.score)
Y.loading = t(Y) %*% T.score %*% solve(t(T.score) %*% T.score)
#matrix coefficients
Wstar = W %*% solve(t(X.loading) %*%W)
B = Wstar %*% t(Y.loading)
Y.fitted <- fitY(X = X, B = B, Mm = Mm, s = s)
Y.fitted
outPLS <- list(X.loading = X.loading,
Y.loading = Y.loading,
B = B,
M = M,
T.score = T.score,
Y.fitted = Y.fitted)
pw <- 0
pb <- progress_bar$new(total = Nsim)
i
#Model the distribution of the X-data
outsim <- sim_XY(out = outPLS, n = n, seed = seed,
post.transformation = post.transformation, A = A)
#Model the distribution of the Y-data
Xsim <- outsim$X_H1
Ysim <- outsim$Y_H1
pv <- eigenTest(X = Xsim, Y = Ysim, A = A, nperm = nperm,
X.scaling = scaling, ...)
Ysim
Xsim
plot(density(X[,1]))
plot(density(Xsim[,1]))
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30),
alpha = 0.05, beta = 0.8,
post.transformation = TRUE, Nsim = 100,
nperm = 100,
seed = 123,
scaling = "auto-scaling", Y.prob = FALSE)
out <- computePower(X = X, Y = Y, A = 4,
alpha = 0.05,
post.transformation = TRUE, Nsim = 100,
nperm = 100,
seed = 123,
scaling = "auto-scaling", Y.prob = FALSE)
out
out <- computePower(X = X, Y = Y, A = 2,
alpha = 0.05,
post.transformation = TRUE, Nsim = 100,
nperm = 100,
seed = 123,
scaling = "auto-scaling", Y.prob = FALSE)
library(fungible)
sim <- monte(seed=123, nvar = 4, nclus = 2,
clus.size = c(50, 50),
eta2=c(0.8, 0.8, 0.01, 0.01), # Higher numbers produce clusters with greater separation on that indicator.
compactness=c(1, 1), #A vector of cluster compactness parameters. The meaning of this option is explained Waller et al. (1999)
sortMeans = FALSE)
X<- sim$data[, -1]
Y <- sim$data[,1]
out <- computePower(X = X, Y = Y, A = 2,
alpha = 0.05,
post.transformation = TRUE, Nsim = 100,
nperm = 100,
seed = 123,
scaling = "auto-scaling", Y.prob = FALSE)
out
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30),
alpha = 0.05, beta = 0.8,
post.transformation = TRUE, Nsim = 100,
nperm = 100,
seed = 123,
scaling = "auto-scaling", Y.prob = FALSE)
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30),
alpha = 0.05, beta = 0.8,
post.transformation = TRUE, Nsim = 100,
nperm = 100,
seed = 123,Y.prob = FALSE)
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30),
alpha = 0.05, beta = 0.8,
post.transformation = TRUE, Nsim = 100,
nperm = 100)
out <- computeSampleSize(X = X, Y = Y, A = 4, n = c(10,30),
alpha = 0.05, beta = 0.8,
post.transformation = TRUE, Nsim = 100,
nperm = 100, Y.prob = FALSE)
